{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FilipSagalara/MasterThesis_SNN_frameworks/blob/main/SPYKETPORCH_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGh8DunR68SS"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install snntorch\n",
        "!pip install git+https://github.com/miladmozafari/SpykeTorch.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "KMNYZ_n4tnIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timer class"
      ],
      "metadata": {
        "id": "UIpuPrZktjG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#add timer\n",
        "import time\n",
        "class Timer:\n",
        "    def __enter__(self):\n",
        "        self.start_time = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.end_time = time.time()\n",
        "        self.execution_time = self.end_time - self.start_time\n",
        "\n",
        "    def get_execution_time(self):\n",
        "        return self.execution_time"
      ],
      "metadata": {
        "id": "8YpKH7hLtRoO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate metrics class\n"
      ],
      "metadata": {
        "id": "3P2Fqb_rMnJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from google.colab import drive\n",
        "\n",
        "class Metrics:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.accuracies = []\n",
        "    self.losses = []\n",
        "    self.prec_micros = []\n",
        "    self.rec_micro = []\n",
        "    self.f1_s_micro = []\n",
        "    self.prec_macro = []\n",
        "    self.rec_macro = []\n",
        "    self.f1_s_macro = []\n",
        "    self.prec_weight = []\n",
        "    self.rec_weight = []\n",
        "    self.f1_s_weight = []\n",
        "    self.classifi_report = []\n",
        "    self.confusion_matrices = []\n",
        "    self.times_per_epoch = []\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.accuracies = []\n",
        "    self.losses = []\n",
        "    self.prec_micros = []\n",
        "    self.rec_micro = []\n",
        "    self.f1_s_micro = []\n",
        "    self.prec_macro = []\n",
        "    self.rec_macro = []\n",
        "    self.f1_s_macro = []\n",
        "    self.prec_weight = []\n",
        "    self.rec_weight = []\n",
        "    self.f1_s_weight = []\n",
        "    self.classifi_report = []\n",
        "    self.confusion_matrices = []\n",
        "    self.times_per_epoch = []\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  def my_custom_loss_func(y_true, y_pred):\n",
        "    diff = np.abs(y_true - y_pred).max()\n",
        "    return np.log1p(diff)\n",
        "\n",
        "\n",
        "\n",
        "  def compute_metrics(self, y_pred, y_test, show = False): #y test refers to true labels\n",
        "    target_names = ['0','1','2','3','4','5','6','7','8','9']\n",
        "    #Compute loss\n",
        "    # loss = log_loss(y_test, y_pred, labels = target_names)\n",
        "    loss = my_custom_loss_func(y_test, y_pred)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    prec_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    rec_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    f1_s_micro = f1_score(y_test, y_pred, average='micro')\n",
        "\n",
        "    prec_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    rec_macro = recall_score(y_test, y_pred, average='macro')\n",
        "    f1_s_macro = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    prec_weight = precision_score(y_test, y_pred, average='weighted')\n",
        "    rec_weight = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1_s_weight = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "\n",
        "    classifi_report = classification_report(y_test, y_pred, target_names=target_names)\n",
        "\n",
        "    # Update metrics\n",
        "    self.accuracies.append(acc)\n",
        "    self.losses.append(loss)\n",
        "    self.prec_micros.append(prec_micro)\n",
        "    self.rec_micro.append(rec_micro)\n",
        "    self.f1_s_micro.append(f1_s_micro)\n",
        "    self.prec_macro.append(prec_macro)\n",
        "    self.rec_macro.append(rec_macro)\n",
        "    self.f1_s_macro.append(f1_s_macro)\n",
        "    self.prec_weight.append(prec_weight)\n",
        "    self.rec_weight.append(rec_weight)\n",
        "    self.f1_s_weight.append(f1_s_weight)\n",
        "\n",
        "    if show:\n",
        "      print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
        "      print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
        "      print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
        "      print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
        "\n",
        "      print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
        "      print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
        "      print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
        "\n",
        "      print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
        "      print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
        "      print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
        "\n",
        "      print('\\nClassification Report\\n')\n",
        "      print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "    return acc, loss, prec_micro, rec_micro, f1_s_micro, prec_macro, rec_macro, f1_s_macro, prec_weight, rec_weight, f1_s_weight, classifi_report\n",
        "\n",
        "  def compute_confusion_matrix(self, y_pred, y_test):\n",
        "    cm = confusion_matrix(y_true=y_test, y_pred = y_pred)\n",
        "    #print(cm)\n",
        "    # cm_display = ConfusionMatrixDisplay(cm).plot()\n",
        "    self.confusion_matrices.append(cm)\n",
        "\n",
        "\n",
        "  def send_to_drive(self, name : str):\n",
        "    if not name:\n",
        "      print(\"Error: Filename is empty\")\n",
        "      return\n",
        "    drive.mount('/content/drive/')\n",
        "    df2 = pd.DataFrame(np.array([self.accuracies, self.prec_micros, self.rec_micro, self.f1_s_micro, self.prec_macro, self.rec_macro, self.f1_s_macro, self.prec_weight, self.rec_weight, self.f1_s_weight, self.times_per_epoch]),\n",
        "    columns=['accuracies', 'prec_micros', 'rec_micro', 'f1_s_micro', 'prec_macro', 'rec_macro', 'f1_s_macro', 'prec_weight','rec_weight', 'f1_s_weight', 'times_per_epoch'])\n",
        "    destination = \"/content/drive/My Drive/\" + name + \".xlsx\"\n",
        "    df2.to_excel(destination)\n",
        "    print(\"Saved at: {}\".format(destination))\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "spyketorch_metrics = Metrics()\n"
      ],
      "metadata": {
        "id": "X3jGTD_LXIbM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oREi4_zo7daA"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wTU2ZnFe7c81"
      },
      "outputs": [],
      "source": [
        "#################################################################################\n",
        "# Reimplementation of the 10-Class Digit Recognition Experiment Performed in:   #\n",
        "# https://arxiv.org/abs/1804.00227                                              #\n",
        "#                                                                               #\n",
        "# Reference:                                                                    #\n",
        "# Mozafari, Milad, et al.,                                                      #\n",
        "# \"Combining STDP and Reward-Modulated STDP in                                  #\n",
        "# Deep Convolutional Spiking Neural Networks for Digit Recognition.\"            #\n",
        "# arXiv preprint arXiv:1804.00227 (2018).                                       #\n",
        "#                                                                               #\n",
        "# Original implementation (in C++/CUDA) is available upon request.              #\n",
        "#################################################################################\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.nn.parameter import Parameter\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from SpykeTorch import snn\n",
        "from SpykeTorch import functional as sf\n",
        "from SpykeTorch import visualization as vis\n",
        "from SpykeTorch import utils\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZYKniMqggbM"
      },
      "source": [
        "# Reproductibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NWYUF2VgiHZ",
        "outputId": "daea8a28-3695-404c-96e4-2625c74d72f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f62ec1287d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#Reproductibility\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD48OeVp7fnh"
      },
      "source": [
        "# use GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ObPKcmZw5eYs"
      },
      "outputs": [],
      "source": [
        "use_cuda = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6mwiqnd3CKf"
      },
      "source": [
        "# Define network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HmSpCMPE7i3_"
      },
      "outputs": [],
      "source": [
        "class MozafariMNIST2018(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MozafariMNIST2018, self).__init__()\n",
        "\n",
        "        self.conv1 = snn.Convolution(6, 30, 5, 0.8, 0.05)\n",
        "        self.conv1_t = 15\n",
        "        self.k1 = 5\n",
        "        self.r1 = 3\n",
        "\n",
        "        self.conv2 = snn.Convolution(30, 250, 3, 0.8, 0.05)\n",
        "        self.conv2_t = 10\n",
        "        self.k2 = 8\n",
        "        self.r2 = 1\n",
        "\n",
        "        self.conv3 = snn.Convolution(250, 200, 5, 0.8, 0.05)\n",
        "\n",
        "        self.stdp1 = snn.STDP(self.conv1, (0.004, -0.003))\n",
        "        self.stdp2 = snn.STDP(self.conv2, (0.004, -0.003))\n",
        "        self.stdp3 = snn.STDP(self.conv3, (0.004, -0.003), False, 0.2, 0.8)\n",
        "        self.anti_stdp3 = snn.STDP(self.conv3, (-0.004, 0.0005), False, 0.2, 0.8)\n",
        "        self.max_ap = Parameter(torch.Tensor([0.15]))\n",
        "\n",
        "        self.decision_map = []\n",
        "        for i in range(10):\n",
        "            self.decision_map.extend([i]*20)\n",
        "\n",
        "        self.ctx = {\"input_spikes\":None, \"potentials\":None, \"output_spikes\":None, \"winners\":None}\n",
        "        self.spk_cnt1 = 0\n",
        "        self.spk_cnt2 = 0\n",
        "\n",
        "        # Metrics\n",
        "        self.predicted_ints = []\n",
        "        self.true_ints = []\n",
        "                \n",
        "    def forward(self, input, max_layer):\n",
        "        input = sf.pad(input.float(), (2,2,2,2), 0)\n",
        "        if self.training:\n",
        "            pot = self.conv1(input)\n",
        "            spk, pot = sf.fire(pot, self.conv1_t, True)\n",
        "            if max_layer == 1:\n",
        "                self.spk_cnt1 += 1\n",
        "                if self.spk_cnt1 >= 500:\n",
        "                    self.spk_cnt1 = 0\n",
        "                    ap = torch.tensor(self.stdp1.learning_rate[0][0].item(), device=self.stdp1.learning_rate[0][0].device) * 2\n",
        "                    ap = torch.min(ap, self.max_ap)\n",
        "                    an = ap * -0.75\n",
        "                    self.stdp1.update_all_learning_rate(ap.item(), an.item())\n",
        "                pot = sf.pointwise_inhibition(pot)\n",
        "                spk = pot.sign()\n",
        "                winners = sf.get_k_winners(pot, self.k1, self.r1, spk)\n",
        "                self.ctx[\"input_spikes\"] = input\n",
        "                self.ctx[\"potentials\"] = pot\n",
        "                self.ctx[\"output_spikes\"] = spk\n",
        "                self.ctx[\"winners\"] = winners\n",
        "                return spk, pot\n",
        "            spk_in = sf.pad(sf.pooling(spk, 2, 2), (1,1,1,1))\n",
        "            pot = self.conv2(spk_in)\n",
        "            spk, pot = sf.fire(pot, self.conv2_t, True)\n",
        "            if max_layer == 2:\n",
        "                self.spk_cnt2 += 1\n",
        "                if self.spk_cnt2 >= 500:\n",
        "                    self.spk_cnt2 = 0\n",
        "                    ap = torch.tensor(self.stdp2.learning_rate[0][0].item(), device=self.stdp2.learning_rate[0][0].device) * 2\n",
        "                    ap = torch.min(ap, self.max_ap)\n",
        "                    an = ap * -0.75\n",
        "                    self.stdp2.update_all_learning_rate(ap.item(), an.item())\n",
        "                pot = sf.pointwise_inhibition(pot)\n",
        "                spk = pot.sign()\n",
        "                winners = sf.get_k_winners(pot, self.k2, self.r2, spk)\n",
        "                self.ctx[\"input_spikes\"] = spk_in\n",
        "                self.ctx[\"potentials\"] = pot\n",
        "                self.ctx[\"output_spikes\"] = spk\n",
        "                self.ctx[\"winners\"] = winners\n",
        "                return spk, pot\n",
        "            spk_in = sf.pad(sf.pooling(spk, 3, 3), (2,2,2,2))\n",
        "            pot = self.conv3(spk_in)\n",
        "            spk = sf.fire(pot)\n",
        "            winners = sf.get_k_winners(pot, 1, 0, spk)\n",
        "            self.ctx[\"input_spikes\"] = spk_in\n",
        "            self.ctx[\"potentials\"] = pot\n",
        "            self.ctx[\"output_spikes\"] = spk\n",
        "            self.ctx[\"winners\"] = winners\n",
        "            output = -1\n",
        "            if len(winners) != 0:\n",
        "                output = self.decision_map[winners[0][0]]\n",
        "            return output\n",
        "        else:\n",
        "            pot = self.conv1(input)\n",
        "            spk, pot = sf.fire(pot, self.conv1_t, True)\n",
        "            if max_layer == 1:\n",
        "                return spk, pot\n",
        "            pot = self.conv2(sf.pad(sf.pooling(spk, 2, 2), (1,1,1,1)))\n",
        "            spk, pot = sf.fire(pot, self.conv2_t, True)\n",
        "            if max_layer == 2:\n",
        "                return spk, pot\n",
        "            pot = self.conv3(sf.pad(sf.pooling(spk, 3, 3), (2,2,2,2)))\n",
        "            spk = sf.fire(pot)\n",
        "            winners = sf.get_k_winners(pot, 1, 0, spk)\n",
        "            output = -1\n",
        "            if len(winners) != 0:\n",
        "                output = self.decision_map[winners[0][0]]\n",
        "            return output\n",
        "    \n",
        "    def stdp(self, layer_idx):\n",
        "        if layer_idx == 1:\n",
        "            self.stdp1(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
        "        if layer_idx == 2:\n",
        "            self.stdp2(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
        "\n",
        "    def update_learning_rates(self, stdp_ap, stdp_an, anti_stdp_ap, anti_stdp_an):\n",
        "        self.stdp3.update_all_learning_rate(stdp_ap, stdp_an)\n",
        "        self.anti_stdp3.update_all_learning_rate(anti_stdp_an, anti_stdp_ap)\n",
        "\n",
        "    def reward(self):\n",
        "        self.stdp3(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
        "\n",
        "    def punish(self):\n",
        "        self.anti_stdp3(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
        "\n",
        "def train_unsupervise(network, data, layer_idx):\n",
        "    network.train()\n",
        "    for i in range(len(data)):\n",
        "        data_in = data[i]\n",
        "        if use_cuda:\n",
        "            data_in = data_in.cuda()\n",
        "        network(data_in, layer_idx)\n",
        "        network.stdp(layer_idx)\n",
        "\n",
        "def train_rl(network, data, target):\n",
        "    network.train()\n",
        "    perf = np.array([0,0,0]) # correct, wrong, silence\n",
        "    for i in range(len(data)):\n",
        "        data_in = data[i]\n",
        "        target_in = target[i]\n",
        "        if use_cuda:\n",
        "            data_in = data_in.cuda()\n",
        "            target_in = target_in.cuda()\n",
        "        d = network(data_in, 3)\n",
        "        if d != -1:\n",
        "            if d == target_in:\n",
        "                perf[0]+=1\n",
        "                network.reward()\n",
        "            else:\n",
        "                perf[1]+=1\n",
        "                network.punish()\n",
        "        else:\n",
        "            perf[2]+=1\n",
        "    return perf/len(data)\n",
        "\n",
        "def test(network : MozafariMNIST2018, data, target):\n",
        "    network.eval()\n",
        "    # Reset predicted ints\n",
        "    predicted_ints = []\n",
        "    true_ints = []\n",
        "    perf = np.array([0,0,0]) # correct, wrong, silence\n",
        "    for i in range(len(data)):\n",
        "        data_in = data[i]\n",
        "        target_in = target[i]\n",
        "        if use_cuda:\n",
        "            data_in = data_in.cuda()\n",
        "            target_in = target_in.cuda()\n",
        "\n",
        "\n",
        "        d = network(data_in, 3)\n",
        "        if d != -1: # no error\n",
        "            if d == target_in:\n",
        "                perf[0]+=1\n",
        "            else:\n",
        "                perf[1]+=1\n",
        "        else:\n",
        "            perf[2]+=1\n",
        "        \n",
        "        # additional metrics\n",
        "        predicted_ints.append(d)\n",
        "        true_ints.append(target_in)\n",
        "\n",
        "\n",
        "    return perf/len(data), predicted_ints, true_ints\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTaZIZPGiQjc"
      },
      "source": [
        "# Define encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vOKNgQc4iRyX"
      },
      "outputs": [],
      "source": [
        "class S1C1Transform:\n",
        "    def __init__(self, filter, timesteps = 15):\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.filter = filter\n",
        "        self.temporal_transform = utils.Intensity2Latency(timesteps)\n",
        "        self.cnt = 0\n",
        "    def __call__(self, image):\n",
        "        if self.cnt % 1000 == 0:\n",
        "            print(self.cnt)\n",
        "        self.cnt+=1\n",
        "        image = self.to_tensor(image) * 255\n",
        "        image.unsqueeze_(0)\n",
        "        image = self.filter(image)\n",
        "        image = sf.local_normalization(image, 8)\n",
        "        temporal_image = self.temporal_transform(image)\n",
        "        return temporal_image.sign().byte()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENkZd2C_3EJe"
      },
      "source": [
        "# Define filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oo3JlmkrI-2h"
      },
      "outputs": [],
      "source": [
        "kernels = [ utils.DoGKernel(3,3/9,6/9),\n",
        "            utils.DoGKernel(3,6/9,3/9),\n",
        "            utils.DoGKernel(7,7/9,14/9),\n",
        "            utils.DoGKernel(7,14/9,7/9),\n",
        "            utils.DoGKernel(13,13/9,26/9),\n",
        "            utils.DoGKernel(13,26/9,13/9)]\n",
        "filter = utils.Filter(kernels, padding = 6, thresholds = 50)\n",
        "s1c1 = S1C1Transform(filter)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txjYrUXagChJ"
      },
      "source": [
        "# Batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lbSE4zqSgDm0"
      },
      "outputs": [],
      "source": [
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdLDKugwc-dD"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q59e0OwfDCtQ",
        "outputId": "30dca298-15f7-4f32-ec17-b53d0a4875ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 90147010.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 23051511.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 25986207.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 13804730.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_root = \"data\"\n",
        "MNIST_train = utils.CacheDataset(torchvision.datasets.MNIST(root=data_root, train=True, download=True, transform = s1c1))\n",
        "MNIST_test = utils.CacheDataset(torchvision.datasets.MNIST(root=data_root, train=False, download=True, transform = s1c1))\n",
        "MNIST_loader = DataLoader(MNIST_train, batch_size=batch_size, shuffle=False)\n",
        "MNIST_testLoader = DataLoader(MNIST_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "mozafari = MozafariMNIST2018()\n",
        "if use_cuda:\n",
        "    mozafari.cuda()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0AWmqgCdBaP"
      },
      "source": [
        "# Show train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CI9R2HUldCr8"
      },
      "outputs": [],
      "source": [
        "# def show_dataset(data, targets, num_of_images = 32):\n",
        "#   rows = 4\n",
        "#   cols = 8\n",
        "#   total_pictures = rows * cols\n",
        "#   plt.figure(figsize=(16,8))\n",
        "#   for img in range(total_pictures):\n",
        "#       plt.subplot(rows, cols, 1+img)\n",
        "#       plt.title(targets[img])\n",
        "#       plt.imshow(data[img])\n",
        "#       plt.axis('off')\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVTXLkwm--Bj"
      },
      "source": [
        "# Set num of epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "YAJnegE3t7fI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3MuCXBi6-_Wz"
      },
      "outputs": [],
      "source": [
        "first_layer_epochs = 2 # originally 2\n",
        "second_layer_epochs = 4 # originally 4\n",
        "third_layer_epochs = 1 # originally 680"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfbpWeSwJH1F"
      },
      "source": [
        "## Training the First Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wj4MBVeJG3t"
      },
      "outputs": [],
      "source": [
        "# Training The First Layer\n",
        "print(\"Training the first layer\")\n",
        "if os.path.isfile(\"saved_l1.net\"):\n",
        "    mozafari.load_state_dict(torch.load(\"saved_l1.net\"))\n",
        "else:\n",
        "    for epoch in range(first_layer_epochs):\n",
        "        print(\"Epoch\", epoch)\n",
        "        iter = 0\n",
        "        for data,targets in MNIST_loader:\n",
        "            print(\"Iteration\", iter)\n",
        "            train_unsupervise(mozafari, data, 1)\n",
        "            print(\"Done!\")\n",
        "            iter+=1\n",
        "    torch.save(mozafari.state_dict(), \"saved_l1.net\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7PBDf2c3a3B"
      },
      "source": [
        "## Training second layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y17rrWTC3OCt"
      },
      "outputs": [],
      "source": [
        "# Training The Second Layer\n",
        "print(\"Training the second layer\")\n",
        "if os.path.isfile(\"saved_l2.net\"):\n",
        "    mozafari.load_state_dict(torch.load(\"saved_l2.net\"))\n",
        "else:\n",
        "    for epoch in range(second_layer_epochs):\n",
        "        print(\"Epoch\", epoch)\n",
        "        iter = 0\n",
        "        for data,targets in MNIST_loader:\n",
        "            print(\"Iteration\", iter)\n",
        "            train_unsupervise(mozafari, data, 2)\n",
        "            print(\"Done!\")\n",
        "            iter+=1\n",
        "    torch.save(mozafari.state_dict(), \"saved_l2.net\")\n",
        "\n",
        "# initial adaptive learning rates\n",
        "apr = mozafari.stdp3.learning_rate[0][0].item()\n",
        "anr = mozafari.stdp3.learning_rate[0][1].item()\n",
        "app = mozafari.anti_stdp3.learning_rate[0][1].item()\n",
        "anp = mozafari.anti_stdp3.learning_rate[0][0].item()\n",
        "\n",
        "adaptive_min = 0\n",
        "adaptive_int = 1\n",
        "apr_adapt = ((1.0 - 1.0 / 10) * adaptive_int + adaptive_min) * apr\n",
        "anr_adapt = ((1.0 - 1.0 / 10) * adaptive_int + adaptive_min) * anr\n",
        "app_adapt = ((1.0 / 10) * adaptive_int + adaptive_min) * app\n",
        "anp_adapt = ((1.0 / 10) * adaptive_int + adaptive_min) * anp\n",
        "\n",
        "# perf\n",
        "best_train = np.array([0.0,0.0,0.0,0.0]) # correct, wrong, silence, epoch\n",
        "best_test = np.array([0.0,0.0,0.0,0.0]) # correct, wrong, silence, epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9WSjBmO3cup"
      },
      "source": [
        "## Training third layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEoPJuvV3U5a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Training The Third Layer\n",
        "print(\"Training the third layer\")\n",
        "\n",
        "for epoch in range(third_layer_epochs):\n",
        "    with Timer() as t:\n",
        "      print(\"Epoch #:\", epoch)\n",
        "      perf_train = np.array([0.0,0.0,0.0])\n",
        "      for data,targets in MNIST_loader:\n",
        "          perf_train_batch = train_rl(mozafari, data, targets)\n",
        "          print(perf_train_batch)\n",
        "          #update adaptive learning rates\n",
        "          apr_adapt = apr * (perf_train_batch[1] * adaptive_int + adaptive_min)\n",
        "          anr_adapt = anr * (perf_train_batch[1] * adaptive_int + adaptive_min)\n",
        "          app_adapt = app * (perf_train_batch[0] * adaptive_int + adaptive_min)\n",
        "          anp_adapt = anp * (perf_train_batch[0] * adaptive_int + adaptive_min)\n",
        "          mozafari.update_learning_rates(apr_adapt, anr_adapt, app_adapt, anp_adapt)\n",
        "          perf_train += perf_train_batch\n",
        "\n",
        "    # Append time on training epoch\n",
        "    spyketorch_metrics.times_per_epoch.append(t.get_execution_time())\n",
        "    perf_train /= len(MNIST_loader)\n",
        "    if best_train[0] <= perf_train[0]:\n",
        "        best_train = np.append(perf_train, epoch)\n",
        "    print(\"Current Train:\", perf_train)\n",
        "    print(\"   Best Train:\", best_train)\n",
        "\n",
        "    \n",
        "    # Model evaluation\n",
        "    for data,targets in MNIST_testLoader:\n",
        "\n",
        "        # Collect data after each epoch validation\n",
        "        perf_test, predicted_ints, true_ints = test(mozafari, data, targets)\n",
        "        spyketorch_metrics.compute_metrics(y_pred= predicted_ints, y_test=true_ints)\n",
        "        spyketorch_metrics.compute_confusion_matrix(y_pred= predicted_ints, y_test=true_ints)\n",
        "        \n",
        "\n",
        "        \n",
        "        if best_test[0] <= perf_test[0]:\n",
        "            best_test = np.append(perf_test, epoch)\n",
        "            torch.save(mozafari.state_dict(), \"saved.net\")\n",
        "\n",
        "\n",
        "        print(\" Current Test:\", perf_test)\n",
        "        print(\"    Best Test:\", best_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send results to drive"
      ],
      "metadata": {
        "id": "0BUQbV_ztMA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spyketorch_metrics.send_to_drive(\"Spyketorch_MNIST\")"
      ],
      "metadata": {
        "id": "BnoFoDP1tLeH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXEq0ih8S0lgdvQafcRrth",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}